"""Cloudflare AI-backed LLM helper.

This module provides a very small wrapper around Cloudflare's AI HTTP API so
the rest of the project does not have to worry about HTTP details.

Configuration is taken from ``constants.py`` (``CLOUDFLARE_ACCOUNT_ID``,
``CLOUDFLARE_API_TOKEN``, and ``CLOUDFLARE_MODEL``). The main entrypoint is
``score_candidate(prompt, model=None, timeout=600)`` which sends a single
``prompt`` string to Cloudflare and returns the raw text produced by the
model. Higher-level code is responsible for building the prompt and parsing
the JSON it returns.

The implementation is intentionally small and deterministic so callers can
catch ``RuntimeError`` and show clear error messages to end users.
"""

from __future__ import annotations

import os
from typing import Optional

import requests

from constants import CLOUDFLARE_API_TOKEN, CLOUDFLARE_ACCOUNT_ID, CLOUDFLARE_MODEL


def is_ollama_installed() -> bool:
	"""Compatibility shim used by :mod:`app` to validate Cloudflare config.

	Historically the project checked for a local Ollama installation using this
	function name. To avoid changing the rest of the code, we now keep the same
	name but interpret it as "is Cloudflare AI configured correctly?".
	"""
	return bool(CLOUDFLARE_API_TOKEN and CLOUDFLARE_ACCOUNT_ID)


def _get_cloudflare_config(model: Optional[str] = None) -> tuple[str, str, str]:
	"""Return ``(account_id, api_token, model_name)`` for Cloudflare calls.

	Values are taken from :mod:`constants` and optionally overridden by the
	``model`` argument.
	"""
	account_id = CLOUDFLARE_ACCOUNT_ID
	api_token = CLOUDFLARE_API_TOKEN
	model_name = model or CLOUDFLARE_MODEL

	if not account_id:
		raise RuntimeError("Missing CLOUDFLARE_ACCOUNT_ID environment variable.")
	if not api_token:
		raise RuntimeError("Missing CLOUDFLARE_API_TOKEN environment variable.")
	if not model_name:
		raise RuntimeError("Cloudflare model name is empty. Set CLOUDFLARE_MODEL or pass model explicitly.")

	return account_id, api_token, model_name


def score_candidate(prompt: str, model: Optional[str] = None, timeout: int = 600) -> str:
	"""Call Cloudflare AI ``/ai/run`` and return the model's text response.

	Args:
		prompt: Fully-formed text prompt that tells the model what to output.
		model: Optional model name. When omitted, ``CLOUDFLARE_MODEL`` is used.
		timeout: Request timeout in seconds.

	Returns:
		The raw text string generated by the model (for this project, a JSON
		object encoded as text).

	Raises:
		RuntimeError: If configuration is missing or the HTTP request fails.
	"""
	account_id, api_token, model_name = _get_cloudflare_config(model)

	url = f"https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/run/{model_name}"
	headers = {
		"Authorization": f"Bearer {api_token}",
		"Content-Type": "application/json",
	}
	# Cloudflare's text generation models accept a single `prompt` string.
	full_prompt = (
		"You are an assistant that scores candidates for a software engineering role. "
		"Return a single STRICT JSON object following the schema described in the prompt. "
		"The JSON must be valid (double-quoted keys/strings, no comments, no trailing commas).\n\n"
		+ prompt
	)
	payload = {"prompt": full_prompt}

	try:
		resp = requests.post(url, headers=headers, json=payload, timeout=timeout)
	except requests.RequestException as exc:
		raise RuntimeError(f"Error calling Cloudflare AI API: {exc}") from exc

	if not resp.ok:
		raise RuntimeError(f"Cloudflare AI API returned HTTP {resp.status_code}: {resp.text[:500]}")

	data = resp.json()
	# Cloudflare's /ai/run response for chat models typically contains the
	# assistant text under 'result.response' or 'result.output'. We first try
	# the more structured chat response, then fall back to simple fields.
	result = data.get("result") or {}
	text: Optional[str] = None

	# Try chat-style responses
	if isinstance(result, dict):
		if "response" in result:
			msg = result["response"]
			if isinstance(msg, dict):
				text = msg.get("content") or msg.get("message")
			elif isinstance(msg, str):
				# Some Cloudflare models return the assistant text as a raw string here
				text = msg
		elif "output" in result:
			msg = result["output"]
			if isinstance(msg, dict):
				text = msg.get("content") or msg.get("message")
			elif isinstance(msg, str):
				text = msg
		elif "output_text" in result:
			text = result.get("output_text")

	if not text and isinstance(result, str):
		text = result

	if not text:
		# Last resort: return the full JSON as a string so callers can inspect.
		text = str(data)

	return text


__all__ = ["is_ollama_installed", "score_candidate"]
